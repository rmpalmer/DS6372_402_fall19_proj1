fit.nb <- train(Attrition ~ ., data = nb.train, method = "naive_bayes", metric = "F_meas", trControl = trnCtrl)
colsToDrop <- c("ID","EmployeeNumber","EmployeeCount","Over18","StandardHours","DailyRate","HourlyRate","MonthlyRate")
nb.train <- train.data %>% select(-colsToDrop)
trnCtrl <- trainControl( method = "repeatedcv",
number = 10, repeats = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE)
fit.nb <- train(Attrition ~ ., data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred <- predict(fit.nb, test.data)
confusionMatrix(pred,test.data$Attrition, positive = "Yes")
F_meas(pred,test.data$Attrition)
names(casedata)
colsToFactor <- c("Attrition","BusinessTravel","Department","EducationField","Gender","JobRole","MaritalStatus","Over18","OverTime","StockOptionLevel","JobLevel","JobInvolvement","Education","EnvironmentSatisfaction","JobSatisfaction","RelationshipSatisfaction","WorkLifeBalance","PerformanceRating")
# Consider "StockOptionLevel","JobLevel","JobInvolvement","Education"; They could be coded as numeric
casedata[,colsToFactor] <- lapply(casedata[,colsToFactor], as.factor)
casedata$logMonthlyIncome <- log(casedata$MonthlyIncome)
casedata$IncomeLt4000 <- ifelse(casedata$MonthlyIncome <= 4000, 1, 0)
casedata$Edu4 <- ifelse(casedata$Education == 4, 1, 0)
casedata$Edu5 <- ifelse(casedata$Education == 5, 1, 0)
casedata$EnvSat1 <- ifelse(casedata$EnvironmentSatisfaction == 1, 1, 0)
casedata$StockOpt0 <- ifelse(casedata$StockOptionLevel == 0, 1, 0)
casedata$StockOpt3 <- ifelse(casedata$StockOptionLevel == 3, 1, 0)
casedata$StockOpt03 <- ifelse(casedata$StockOptionLevel %in% c(0,3), 1, 0)
casedata$JobLev1 <- ifelse(casedata$JobLevel == 1, 1, 0)
casedata$JobInv1 <- ifelse(casedata$JobInvolvement == 1, 1, 0)
casedata$WorkLife1 <- ifelse(casedata$WorkLifeBalance == 1, 1, 0)
casedata$DistHomeFactor <- as.factor(
case_when(casedata$DistanceFromHome <= 10 ~ "Close",
casedata$DistanceFromHome > 10 & casedata$DistanceFromHome <= 20 ~ "Medium",
casedata$DistanceFromHome > 20 ~ "Far")
)
casedata$HourDay <- casedata$HourlyRate * 8
names(casedata)
names(casedata)[,c(36:49)]
names(casedata)[c(36:49)]
names(casedata)[c(37:49)]
length(casedata)
nb.train <- train.data %>% select(-colsToDrop, -names(casedata)[c(37:length(casedata))])
c(37:length(casedata))
names(casedata)[c(37:length(casedata))]
colsToDrop2 <- names(casedata)[c(37:length(casedata))]
str(colsToDrop2)
str(colsToDrop)
colsToDrop + colsToDrop2
nb.train <- train.data %>% select(-c(colsToDrop,colsToDrop2))
library(caret)
set.seed(42)
train_ind <- createDataPartition(y = casedata$ID,
p = 0.8,
list = FALSE)
train.data <- casedata[train_ind,]
test.data <- casedata[-train_ind,]
colsToDrop <- c("ID","EmployeeNumber","EmployeeCount","Over18","StandardHours","DailyRate","HourlyRate","MonthlyRate")
colsToDrop2 <- names(casedata)[c(37:length(casedata))]
nb.train <- train.data %>% select(-c(colsToDrop,colsToDrop2))
trnCtrl <- trainControl( method = "repeatedcv",
number = 10, repeats = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE)
fit.nb <- train(Attrition ~ ., data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred <- predict(fit.nb, test.data)
confusionMatrix(pred,test.data$Attrition, positive = "Yes")
F_meas(pred,test.data$Attrition)
fit.nb.02 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel, data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred.02 <- predict(fit.nb, test.data)
confusionMatrix(pred.02,test.data$Attrition, positive = "Yes")
F_meas(pred.02,test.data$Attrition)
pred.02 <- predict(fit.nb.02, test.data)
fit.nb.02 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel, data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred.02 <- predict(fit.nb.02, test.data)
confusionMatrix(pred.02,test.data$Attrition, positive = "Yes")
F_meas(pred.02,test.data$Attrition)
summary(fit.nb.02)
fit.nb.02
fit.nb.03 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel, data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
fit.nb.03 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel, data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred.03 <- predict(fit.nb.03, test.data)
pred.03 <- predict(fit.nb.03, test.data)
confusionMatrix(pred.03,test.data$Attrition, positive = "Yes")
F_meas(pred.03,test.data$Attrition)
fit.nb.03 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel, data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred.03 <- predict(fit.nb.03, test.data)
confusionMatrix(pred.03,test.data$Attrition, positive = "Yes")
F_meas(pred.03,test.data$Attrition)
fit.nb.04 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel + YearsInCurrentRole, data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred.04 <- predict(fit.nb.04, test.data)
confusionMatrix(pred.04,test.data$Attrition, positive = "Yes")
F_meas(pred.04,test.data$Attrition)
colsToDrop <- c("ID","EmployeeNumber","EmployeeCount","Over18","StandardHours","DailyRate","HourlyRate","MonthlyRate")
colsToDrop2 <- names(casedata)[c(37:length(casedata))]
nb.train.01 <- train.data %>% select(-c(colsToDrop,colsToDrop2))
trnCtrl <- trainControl( method = "repeatedcv",
number = 10, repeats = 5,
summaryFunction = twoClassSummary,
classProbs = TRUE)
fit.nb <- train(Attrition ~ ., data = nb.train.01, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred <- predict(fit.nb, test.data)
confusionMatrix(pred,test.data$Attrition, positive = "Yes")
F_meas(pred,test.data$Attrition)
fit.nb.02 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel,
data = nb.train.01,
method = "naive_bayes",
metric = "Spec",
trControl = trnCtrl)
pred.02 <- predict(fit.nb.02, test.data)
confusionMatrix(pred.02,test.data$Attrition, positive = "Yes")
F_meas(pred.02,test.data$Attrition)
fit.nb.03 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel,
data = nb.train.01,
method = "naive_bayes",
metric = "Spec",
trControl = trnCtrl)
pred.03 <- predict(fit.nb.03, test.data)
confusionMatrix(pred.03,test.data$Attrition, positive = "Yes")
F_meas(pred.03,test.data$Attrition)
fit.nb.04 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel + YearsInCurrentRole,
data = nb.train.01,
method = "naive_bayes",
metric = "Spec",
trControl = trnCtrl)
pred.04 <- predict(fit.nb.04, test.data)
confusionMatrix(pred.04,test.data$Attrition, positive = "Yes")
F_meas(pred.04,test.data$Attrition)
fit.nb.05 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel + YearsInCurrentRole, data = nb.train, method = "naive_bayes", metric = "Spec", trControl = trnCtrl)
pred.05 <- predict(fit.nb.05, test.data)
confusionMatrix(pred.05,test.data$Attrition, positive = "Yes")
F_meas(pred.05,test.data$Attrition)
nb.train.02 <- train.data %>% select(-c(colsToDrop))
fit.nb.05 <- train(Attrition ~ OverTime + MaritalStatus + StockOptionLevel + JobLevel + YearsInCurrentRole + IncomeLt4000,
data = nb.train.02,
method = "naive_bayes",
metric = "Spec",
trControl = trnCtrl)
pred.05 <- predict(fit.nb.05, test.data)
confusionMatrix(pred.05,test.data$Attrition, positive = "Yes")
F_meas(pred.05,test.data$Attrition)
table(casedata$JobLevel,casedata$StockOptionLevel)
fit.nb$modelInfo
fit.nb$call
fit.nb$finalModel
fit.nb$dots
fit.nb$levels
fit.nb$coefnames
data.frame("pred05",sensitivity(pred,test.data$Attrition),specificity(pred,test.data$Attrition), F_meas(pred,test.data$Attrition),fit.nb$coefnames)
data.frame("pred05",sensitivity(pred,test.data$Attrition),specificity(pred,test.data$Attrition), F_meas(pred,test.data$Attrition),paste0(fit.nb$coefnames))
data.frame("pred05",sensitivity(pred,test.data$Attrition),specificity(pred,test.data$Attrition), F_meas(pred,test.data$Attrition),paste0(fit.nb.02$coefnames))
paste0(fit.nb.02$coefnames)
data.frame("pred05",sensitivity(pred,test.data$Attrition),specificity(pred,test.data$Attrition), F_meas(pred,test.data$Attrition),paste0(fit.nb.02$coefnames,collapse = "+"))
testing <- data.frame("pred05",sensitivity(pred,test.data$Attrition),specificity(pred,test.data$Attrition), F_meas(pred,test.data$Attrition),paste0(fit.nb.02$coefnames,collapse = "+"))
names(testing) <- c("name", "sens", "spec", "F1", "attributes")
View(testing)
"pred" %in% testing$name
"pred05" %in% testing$name
testing <- rbind(testing, c("pred04", 1, 1, 1, "hi"))
testing <- rbind(testing, data.frame("pred04", 1, 1, 1, "hi"))
test2 <- data.frame("pred04", 1, 1, 1, "hi"))
test2 <- data.frame("pred04", 1, 1, 1, "hi")
names(test2) <- c("name", "sens", "spec", "F1", "attributes")
testing <- rbind(testing, test2)
"pred05" %in% testing$name
"pred0" %in% testing$name
"pred04" %in% testing$name
AR1<-arima.sim(list(ar=c(0.8)),10000) #AR1 is just a vector of values.  They will be centered around 0 unless we add a shift or include some other source of variation like a predictor.
par(mfrow=c(1,3))
plot(1:10000,AR1,type="l")
acf(AR1,main="ACF")
pacf(AR1,main="PACF")
set.seed(314159)
AR1<-arima.sim(list(ar=c(0.8)),50)
par(mfrow=c(1,3))
plot(1:10000,AR1,type="l")
set.seed(314159)
AR1<-arima.sim(list(ar=c(0.8)),50)
par(mfrow=c(1,3))
plot(1:50,AR1,type="l")
acf(AR1,main="ACF")
pacf(AR1,main="PACF")
AR2<-arima.sim(list(ar=c(0.8)),50)
par(mfrow=c(1,3))
plot(1:50,AR2,type="l")
acf(AR2,main="ACF")
pacf(AR2,main="PACF")
AR3<-arima.sim(list(ar=c(0.8)),50)
par(mfrow=c(1,3))
plot(1:50,AR3,type="l")
acf(AR3,main="ACF")
pacf(AR3,main="PACF")
AR1<-arima.sim(list(ar=c(0.8)),10000) #AR1 is just a vector of values.  They will be centered around 0 unless we add a shift or include some other source of variation like a predictor.
par(mfrow=c(1,3))
plot(1:10000,AR1,type="l")
acf(AR1,main="ACF")
pacf(AR1,main="PACF")
AR1<-arima.sim(list(ar=c(0.0)),10000) #AR1 is just a vector of values.  They will be centered around 0 unless we add a shift or include some other source of variation like a predictor.
par(mfrow=c(1,3))
plot(1:10000,AR1,type="l")
acf(AR1,main="ACF")
pacf(AR1,main="PACF")
rho1<-.8
rho2<-.6
a1<-(rho1*(1-rho2)/(1-rho1^2))
a2<-(rho2-rho1^2)/(1-rho1^2)
AR2<-arima.sim(list(ar=c(a1,a2)),10000)
par(mfrow=c(1,3))
plot(1:10000,AR2,type="l")
acf(AR2)
pacf(AR2,main="PACF")
a1<-1.5
a2<--1.21
a3<-.46
AR3<-arima.sim(list(ar=c(a1,a2,a3)),10000)
par(mfrow=c(1,3))
plot(1:10000,AR3,type="l")
acf(AR3,main="ACF")
pacf(AR3,main="PACF")
a1<-1.5
a2<--1.21
a3<-.46
b1<--.2
b2<--.9
ARMA32<-arima.sim(list(ar=c(a1,a2,a3),ma=c(b1,b2)),10000)
par(mfrow=c(1,3))
plot(1:10000,ARMA32,type="l")
acf(ARMA32,main="ACF")
pacf(ARMA32,main="PACF")
b1<- .2
b2<- .9
MA2<-arima.sim(list(ma=c(b1,b2)),10000)
par(mfrow=c(1,3))
plot(1:10000,MA2,type="l")
acf(MA2,main="ACF")
pacf(MA2,main="PACF")
#setwd("D:/MSDS6372/HWMark")
setwd("/Volumes/TRAVELDRIVE/MSDS6372/HWMark")
#setwd("D:/MSDS6372/HWMark")
library(tseries)
library(forecast)
library(ggplot2)
bills<-read.csv("ElectricBill.csv")
head(bills)
bills$DateIndex<-1:nrow(bills)
ggplot()+geom_line(data=bills,aes(x=DateIndex,y=Bill))
#setwd("D:/MSDS6372/HWMark")
library(tseries)
library(forecast)
library(ggplot2)
bills<-read_csv("ElectricBill.csv")
library(knitr)
library(rmdformats)
library(tidyverse)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
cache=FALSE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
#setwd("D:/MSDS6372/HWMark")
library(tseries)
library(forecast)
library(ggplot2)
bills<-read_csv("ElectricBill.csv")
head(bills)
bills$DateIndex<-1:nrow(bills)
ggplot()+geom_line(data=bills,aes(x=DateIndex,y=Bill))
attach(bills)
Acf(Bill)
Pacf(Bill)
library(car)
durbinWatsonTest(lm(Bill~1),max.lag=4)
AR1<-arima(Bill,order=c(1,0,0))
AR2<-arima(Bill,order=c(2,0,0))
AR3<-arima(Bill,order=c(3,0,0))
AR1<-arima(Bill,order=c(1,0,0))
AR2<-arima(Bill,order=c(2,0,0))
AR3<-arima(Bill,order=c(3,0,0))
tsdisplay(residuals(AR1),lag.max=15,main="AR(1) Resid. Diagnostics")
#setwd("D:/MSDS6372/HWMark")
library(tseries)
library(forecast)
library(car)
bills<-read_csv("ElectricBill.csv")
head(bills)
bills$DateIndex<-1:nrow(bills)
ggplot()+geom_line(data=bills,aes(x=DateIndex,y=Bill))
AR1<-arima(Bill,order=c(1,0,0))
tsdisplay(residuals(AR1),lag.max=15,main="AR(1) Resid. Diagnostics")
AR2<-arima(Bill,order=c(2,0,0))
tsdisplay(residuals(AR2),lag.max=15,main="AR(2) Resid. Diagnostics")
AR3<-arima(Bill,order=c(3,0,0))
tsdisplay(residuals(AR3),lag.max=15,main="AR(3) Resid. Diagnostics")
AR4<-arima(Bill,order=c(4,0,0))
tsdisplay(residuals(AR4),lag.max=15,main="AR(4) Resid. Diagnostics")
AR5<-arima(Bill,order=c(5,0,0))
tsdisplay(residuals(AR5),lag.max=15,main="AR(5) Resid. Diagnostics")
AIC(AR1)
AIC(AR2)
AIC(AR3)
AIC(AR4)
AIC(AR5)
par(mfrow=c(1,2))
plot(forecast(AR4,h=10))
points(1:length(Bill),fitted(AR4),type="l",col="blue")
plot(forecast(AR1,h=10))
points(1:length(Bill),fitted(AR1),type="l",col="blue")
par(mfrow=c(2,1))
plot(forecast(AR4,h=10))
points(1:length(Bill),fitted(AR4),type="l",col="blue")
plot(forecast(AR1,h=10))
points(1:length(Bill),fitted(AR1),type="l",col="blue")
plot(forecast(AR4,h=100))
points(1:length(Bill),fitted(AR4),type="l",col="blue")
holdout.test<-window(ts(Bill),start=36)
train<-Bill[1:35]
AR4.model<-arima(train,order=c(4,0,0))
tsdisplay(residuals(AR4.model),lag.max=15,main="Resid. Diagnostics of AR4")
plot(forecast(AR4.model,h=5))
points(1:length(train),fitted(AR4.model),type="l",col="blue")
points(1:40,Bill,type="l")
holdout.test<-window(ts(Bill),start=36)
train<-Bill[1:35]
AR4.model<-arima(train,order=c(4,0,0))
tsdisplay(residuals(AR4.model),lag.max=15,main="Resid. Diagnostics of AR4")
plot(forecast(AR4.model,h=5))
points(1:length(train),fitted(AR4.model),type="l",col="blue")
points(1:40,Bill,type="l")
casts.ar4<-forecast(AR4.model,h=5)
accuracy(casts.ar4,Bill[36:40])
lambda <- 10^seq(-3, 3, length = 100)
lambda <- 10^seq(-3, 3, length = 100)
library(knitr)
library(rmdformats)
library(tidyverse)
library(kableExtra)
library(doParallel)
workers <- makeCluster(8L)
registerDoParallel(workers)
## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
cache=FALSE,
prompt=FALSE,
tidy=TRUE,
comment=NA,
message=FALSE,
warning=FALSE)
opts_knit$set(width=75)
hitters <- read_csv("Hitters.csv")
str(hitters)
head(hitters)
barPlot <- function(df = hitters, x) {
ggplot(df, aes_string(x = x)) +
geom_bar()
}
propPlot <- function(df = hitters, x, y) {
ggplot(df, aes_string(x = x, fill = y)) +
geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent)
}
fancyTable <- function(df = hitters, x) {
df %>% group_by_at(x) %>%
summarise(Count = n(), Proportion = scales::percent(n()/dim(df)[1])) %>%
kable() %>% kable_styling(full_width = FALSE)
}
vioPlot <- function(df = hitters, x, y) {
ggplot(df, aes_string(x = x, y = y, fill = x)) +
geom_violin(show.legend = FALSE) +
geom_boxplot(width = 0.20, show.legend = FALSE) +
stat_summary(fun.y=mean, geom="point",
shape=5, size=4, color = "black",
show.legend = FALSE)
}
histPlot <- function(df = hitters, x){
ggplot(df,aes_string(x)) + geom_histogram(bins = 30)
}
scatterPlot <- function(df = hitters, x, y = "Salary") {
ggplot(df, aes_string(x = x, y = y)) + geom_point() + geom_smooth(method="lm")
}
colsToFactor <- c("League","Division","NewLeague")
hitters[,colsToFactor] <- lapply(hitters[,colsToFactor], as.factor)
hitters <- hitters %>% filter(!is.na(Salary))
hitters$logSalary <- log(hitters$Salary)
hitters[,-Salary]
hitters[,-"Salary"]
hitters[,-c("Salary")]
library(caret)
library(caret)
lambda <- 10^seq(-3, 3, length = 100)
set.seed(314159)
lasso <- train(
logSalary ~., data = hitters %>% select(-Salary), method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)
lasso <- train(
logSalary ~., data = hitters %>% select(-Salary), method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
colsToFactor <- c("League","Division","NewLeague")
hitters[,colsToFactor] <- lapply(hitters[,colsToFactor], as.factor)
hitters <- hitters %>% filter(!is.na(Salary))
hitters$logSalary <- log(hitters$Salary)
lasso <- train(
logSalary ~., data = hitters %>% select(-Salary), method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)
lasso <- train(
logSalary ~., data = hitters %>% select(-Salary), method = "glmnet",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(hitters)
# Model prediction performance
data.frame(
RMSE = RMSE(exp(predictions), hitters$Salary),
Rsquare = R2(exp(predictions), hitters$Salary)
)
library(caret)
lambda <- 10^seq(-3, 3, length = 100)
set.seed(314159)
lasso <- train(
logSalary ~., data = hitters %>% select(-Salary), method = "glmnet",
trControl = trainControl("cv", number = 5),
tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(hitters)
# Model prediction performance
perf<-data.frame(
RMSE = RMSE(exp(predictions), hitters$Salary),
Rsquare = R2(exp(predictions), hitters$Salary)
)
View(perf)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
read_chunk("Project1_HittersData_WA.Rmd")
<<Salary>>
read_chunk("Project1_HittersData_WA.Rmd")
<<Salary>>
<<salary>>
<<salary>>
<<salary, cache=FALSE>>
library(knitr)
library(rmdformats)
library(tidyverse)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
hitters <- read_csv("Hitters.csv")
colsToFactor <- c("League","Division","NewLeague")
hitters[,colsToFactor] <- lapply(hitters[,colsToFactor], as.factor)
hitters <- hitters %>% filter(!is.na(Salary))
hitters$logSalary <- log(hitters$Salary)
ggplot(hitters,aes(Salary)) + geom_histogram(aes(y=..density..), bins = 30) +
stat_function(fun=dnorm,
color="red",
args=list(mean=mean(hitters$Salary),
sd=sd(hitters$Salary)))
summary(hitters$Salary)
?na
?summary
Hitters <- read.csv("Hitters.csv",header=T)
setwd("C:/Users/William/OneDrive/MSDS_6372_AppliedStatistics/DS6372_402_fall19_proj1")
Hitters <- read.csv("Hitters.csv",header=T)
# Libraries
library (glmnet)
# Remove NAs
Hitters <- na.omit(Hitters)
# Log Salary and Career Attributes
Hitters$Salary = log(Hitters$Salary)
Hitters$CAtBat = log(Hitters$CAtBat+1)
Hitters$CHits = log(Hitters$CHits+1)
Hitters$CHmRun = log(Hitters$CHmRun+1)
Hitters$CRBI = log(Hitters$CRBI+1)
Hitters$CRuns = log(Hitters$CRuns+1)
Hitters$CWalks = log(Hitters$CWalks+1)
# Setup X/Y stuff for glmnet
x=model.matrix(Salary~.,Hitters)[,-1]
View(x)
y=Hitters$Salary
grid=10^seq(10,-2,length =100)
# Create filter for train/test
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
# Build model using training data
lasso.mod=glmnet(x[train,],y[train],alpha =1,lambda=grid)
plot(lasso.mod)
# find lamda that results in smallest cross validation error and the MSE
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha =1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod ,s=bestlam,newx=x[test ,])
testMSE_LASSO<-mean((y.test-lasso.pred)^2)
testMSE_LASSO
# Refit Lasso against full data set and output coefficients
out=glmnet(x,y,alpha=1,lambda=grid)
coef(out,s=bestlam)
install.packages("FSA")
