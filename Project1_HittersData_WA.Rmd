---
title: "Stats2 Project 1"
date: "`r Sys.Date()`"
author: "William Arnost, Dan Crouthamel"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
---


```{r knitr_init, echo=FALSE, cache=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(tidyverse)
library(kableExtra)
library(gridExtra)
library(doParallel)

workers <- makeCluster(8L)
registerDoParallel(workers)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

# Hitters Data Set  

Source: https://www.kaggle.com/floser/hitters  

The following text is verbatim from the website. It is mostly so I don't have to go look at it constantly, we should delete or rephrase for the final submission.  

This dataset is part of the R-package ISLR and is used in the related book by G. James et al. (2013) "An Introduction to Statistical Learning with applications in R" to demonstrate how Ridge regression and the LASSO are performed using R.  

This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.

Format A data frame with 322 observations of major league players on the following 20 variables. AtBat Number of times at bat in 1986 Hits Number of hits in 1986 HmRun Number of home runs in 1986 Runs Number of runs in 1986 RBI Number of runs batted in in 1986 Walks Number of walks in 1986 Years Number of years in the major leagues CAtBat Number of times at bat during his career CHits Number of hits during his career CHmRun Number of home runs during his career CRuns Number of runs during his career CRBI Number of runs batted in during his career CWalks Number of walks during his career League A factor with levels A and N indicating player’s league at the end of 1986 Division A factor with levels E and W indicating player’s division at the end of 1986 PutOuts Number of put outs in 1986 Assists Number of assists in 1986 Errors Number of errors in 1986 Salary 1987 annual salary on opening day in thousands of dollars NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987  

Please cite/acknowledge: Games, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York.  

## Dictionary  

Again, not original content, this is from the website I am just making it more readable.  

Format A data frame with 322 observations of major league players on the following 20 variables.  
AtBat Number of times at bat in 1986  
Hits Number of hits in 1986  
HmRun Number of home runs in 1986  
Runs Number of runs in 1986  
RBI Number of runs batted in in 1986  
Walks Number of walks in 1986  
Years Number of years in the major leagues  
CAtBat Number of times at bat during his career  
CHits Number of hits during his career  
CHmRun Number of home runs during his career  
CRuns Number of runs during his career  
CRBI Number of runs batted in during his career  
CWalks Number of walks during his career  
League A factor with levels A and N indicating player’s league at the end of 1986  
Division A factor with levels E and W indicating player’s division at the end of 1986  
PutOuts Number of put outs in 1986  
Assists Number of assists in 1986  
Errors Number of errors in 1986  
Salary 1987 annual salary on opening day in thousands of dollars  
NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987  

## Data Import  

The data has 322 observations and 20 variables. 17 numeric variables and 3 character variables.  
```{r import}
hitters <- read_csv("Hitters.csv")
str(hitters)
head(hitters)
```

## Check for missing data  

Salary seems to be missing for 17ish percent of the data, all other fields are present. I should look at the data dictionary its strange that the dependent variable has that much missing here.  

```{r missing}
library(naniar)
gg_miss_var(hitters, show_pct=TRUE) + labs(title="Percent Missing by Data Field") +
  theme(legend.position = "none",plot.title = element_text(hjust = 0.5),axis.title.y=element_text(angle=0,vjust=1))
```

## Helper Functions  

These are to just make EDA a little faster.  

```{r helper}
barPlot <- function(df = hitters, x) {
  ggplot(df, aes_string(x = x)) +
    geom_bar() 
}

propPlot <- function(df = hitters, x, y) {
  ggplot(df, aes_string(x = x, fill = y)) +
    geom_bar(position = "fill") +
    scale_y_continuous(labels = scales::percent)
}

fancyTable <- function(df = hitters, x) {
  df %>% group_by_at(x) %>% 
    summarise(Count = n(), Proportion = scales::percent(n()/dim(df)[1])) %>% 
    kable() %>% kable_styling(full_width = FALSE)
}

vioPlot <- function(df = hitters, x, y) {
  ggplot(df, aes_string(x = x, y = y, fill = x)) + 
    geom_violin(show.legend = FALSE) + 
    geom_boxplot(width = 0.20, show.legend = FALSE) + 
    stat_summary(fun.y=mean, geom="point",
                 shape=5, size=4, color = "black",
                 show.legend = FALSE)
}

histPlot <- function(df = hitters, x){
  ggplot(df,aes_string(x)) + geom_histogram(bins = 30) 
}

scatterPlot <- function(df = hitters, x, y = "Salary") {
  ggplot(df, aes_string(x = x, y = y)) + geom_point() + geom_smooth(method="lm")
}
```

## Exploratory Data Analysis  

### Pre-Processing  

We can't do any prediction on missing values. I am choosing to eliminate them now. A sub analyis would look at whether or not independent variables of missing observations have a different distribution.  

```{r preprocess}
colsToFactor <- c("League","Division","NewLeague")
hitters[,colsToFactor] <- lapply(hitters[,colsToFactor], as.factor)
hitters <- hitters %>% filter(!is.na(Salary))
hitters$logSalary <- log(hitters$Salary)
```


### Salary  

Salary is non-normal. May benefit from transformation. 59 NAs. Min Salary ~67.5K, not bad.  

```{r Salary}
a <- ggplot(hitters,aes(Salary)) + geom_histogram(aes(y=..density..), bins = 30) + 
   stat_function(fun=dnorm,
      color="red",
      args=list(mean=mean(hitters$Salary), 
      sd=sd(hitters$Salary))) 
a
summary(hitters$Salary)

b <- ggplot(hitters,aes(logSalary)) + geom_histogram(aes(y=..density..), bins = 30) + 
   stat_function(fun=dnorm,
      color="red",
      args=list(mean=mean(hitters$logSalary), 
      sd=sd(hitters$logSalary))) 
b

c <- vioPlot(x = "League",y="Salary")
c

d <- vioPlot(x = "Division",y="Salary")
d
```
```{r}
grid.arrange(a,b,c,d,ncol=2)
```

### AtBat  

Number of times at bat in 1986. Positive influence on Salary.    

```{r AtBat}
histPlot(x="AtBat")
summary(hitters$AtBat)
vioPlot(x = "League",y="AtBat")
vioPlot(x = "Division",y="AtBat")
scatterPlot(x = "AtBat")
```

### Hits  

Hits Number of hits in 1986. Positive influence on Salary    

```{r Hits}
histPlot(x="Hits")
summary(hitters$Hits)
vioPlot(x = "League",y="Hits")
vioPlot(x = "Division",y="Hits")
scatterPlot(x = "Hits")
```

### HmRun  

HmRun Number of home runs in 1986  

```{r HmRun}
histPlot(x="HmRun")
summary(hitters$HmRun)
vioPlot(x = "League",y="HmRun")
vioPlot(x = "Division",y="HmRun")
scatterPlot(x = "HmRun")
```

### Runs    

Runs Number of runs in 1986   

```{r Runs}
histPlot(x="Runs")
summary(hitters$Runs)
vioPlot(x = "League",y="Runs")
vioPlot(x = "Division",y="Runs")
scatterPlot(x = "Runs")
```

### RBI  

RBI Number of runs batted in in 1986   

```{r RBI}
histPlot(x="RBI")
summary(hitters$RBI)
vioPlot(x = "League",y="RBI")
vioPlot(x = "Division",y="RBI")
scatterPlot(x = "RBI")
```

### Walks  

Walks Number of walks in 1986  

```{r Walks}
histPlot(x="Walks")
summary(hitters$Walks)
vioPlot(x = "League",y="Walks")
vioPlot(x = "Division",y="Walks")
scatterPlot(x = "Walks")
```

### Years    

Years Number of years in the major leagues    

```{r Years}
histPlot(x="Years")
summary(hitters$Years)
vioPlot(x = "League",y="Years")
vioPlot(x = "Division",y="Years")
scatterPlot(x = "Years")
```

### CAtBat  

CAtBat Number of times at bat during his career   

```{r CAtBat}
histPlot(x="CAtBat")
summary(hitters$CAtBat)
vioPlot(x = "League",y="CAtBat")
vioPlot(x = "Division",y="CAtBat")
scatterPlot(x = "CAtBat")
```

### CHits   

CHits Number of hits during his career   

```{r CHits}
histPlot(x="CHits")
summary(hitters$CHits)
vioPlot(x = "League",y="CHits")
vioPlot(x = "Division",y="CHits")
scatterPlot(x = "CHits")
```

### CHmRun   

CHmRun Number of home runs during his career   

```{r CHmRun}
histPlot(x="CHmRun")
summary(hitters$CHmRun)
vioPlot(x = "League",y="CHmRun")
vioPlot(x = "Division",y="CHmRun")
scatterPlot(x = "CHmRun")
```

### CRuns   

CRuns Number of runs during his career    

```{r CRuns}
histPlot(x="CRuns")
summary(hitters$CRuns)
vioPlot(x = "League",y="CRuns")
vioPlot(x = "Division",y="CRuns")
scatterPlot(x = "CRuns")
```

### CRBI   

CRBI Number of runs batted in during his career     

```{r CRBI}
histPlot(x="CRBI")
summary(hitters$CRBI)
vioPlot(x = "League",y="CRBI")
vioPlot(x = "Division",y="CRBI")
scatterPlot(x = "CRBI")
```

### CWalks   

CWalks Number of walks during his career     

```{r CWalks}
histPlot(x="CWalks")
summary(hitters$CWalks)
vioPlot(x = "League",y="CWalks")
vioPlot(x = "Division",y="CWalks")
scatterPlot(x = "CWalks")
```

### PutOuts     

PutOuts Number of put outs in 1986     

```{r PutOuts}
histPlot(x="PutOuts")
summary(hitters$PutOuts)
vioPlot(x = "League",y="PutOuts")
vioPlot(x = "Division",y="PutOuts")
scatterPlot(x = "PutOuts")
```

### Assists 

Assists Number of assists in 1986  

```{r Assists}
histPlot(x="Assists")
summary(hitters$Assists)
vioPlot(x = "League",y="Assists")
vioPlot(x = "Division",y="Assists")
scatterPlot(x = "Assists")
```

### Errors  

Errors Number of errors in 1986    

```{r Errors}
histPlot(x="Errors")
summary(hitters$Errors)
vioPlot(x = "League",y="Errors")
vioPlot(x = "Division",y="Errors")
scatterPlot(x = "Errors")
```

### League  

League A factor with levels A and N indicating player’s league at the end of 1986  

```{r League}
fancyTable(x="League")
barPlot(x="League")
vioPlot(x = "League",y="Salary")
```

### Division  

Division A factor with levels E and W indicating player’s division at the end of 1986    

```{r Division}
fancyTable(x="Division")
barPlot(x="Division")
vioPlot(x = "Division",y="Salary")
```

### NewLeague  

NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987    

```{r NewLeague}
fancyTable(x="NewLeague")
barPlot(x="NewLeague")
vioPlot(x = "NewLeague",y="Salary")
```

### Correlation Plot  

```{r correlation}
library(GGally)
#ggpairs(data = hitters, 
#              mapping = aes(color = League),
#              columns = c("Salary","AtBat","Hits","HmRun","Runs","RBI","Walks","Years",
#                          "CAtBat","CHits","CHmRun","CRuns","CRBI","CWalks","PutOuts",
#                          "Assists","Errors"))

# ggpairs(data = hitters, 
#               mapping = aes(color = League),
#               columns = c("Salary","CAtBat","CHits","CHmRun","CRuns","CRBI","CWalks","PutOuts",
#                           "Assists","Errors"))
# 
# ggpairs(data = hitters, 
#               mapping = aes(color = League),
#               columns = c("Salary","AtBat","Hits","HmRun","Runs","RBI","Walks","Years"))

library(corrplot)
cors <- cor(hitters %>% keep(is.numeric))
corrplot(cors, method="circle")
```


## LASSO Selecction

```{r}
library(caret)
lambda <- 10^seq(-3, 3, length = 100)
set.seed(314159)

lasso <- train(
  logSalary ~., data = hitters %>% select(-Salary), method = "glmnet",
  trControl = trainControl("cv", number = 5),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
  )
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(hitters)
# Model prediction performance
perf<-data.frame(
  RMSE = RMSE(exp(predictions), hitters$Salary),
  Rsquare = R2(exp(predictions), hitters$Salary)
)
```

## LASSO Selection 2 ##
As mentioned above, we see some justification for logging Salary. Additionally, there is benefit to logging the career attributes as well, those the start with 'C'. Let's compare before and after scatter plots.

* Scatter plots using Salary
```{r}
ggpairs(data = hitters, columns = c('Salary','CAtBat','CHits','CHmRun','CRuns','CRBI','CWalks'))
```

* Scatter plots using log of Salary
```{r}
ggpairs(data = hitters, columns = c('logSalary','CAtBat','CHits','CHmRun','CRuns','CRBI','CWalks'))
```

* Scatter plots using log of Salary and log of Career Attributes. Important, I'm adding 1 to the log to avoid 0s/NAs.
```{r}
hitters$logCAtBat <- log(hitters$CAtBat+1)
hitters$logCHits <- log(hitters$CHits+1)
hitters$logCHmRun <- log(hitters$CHmRun+1)
hitters$logCRuns <- log(hitters$CRuns+1)
hitters$logCRBI <- log(hitters$CRBI+1)
hitters$logCWalks <- log(hitters$CWalks+1)

ggpairs(data = hitters, columns = c('logSalary','logCAtBat','logCHits','logCHmRun','logCRuns','logCRBI','logCWalks'))
```

Above we can see that the correlation is stronger with the 3rd plot and we generally see a better linear fit.

*Linear Model Selection*

With regards to model selection, our first attempt was to use Lasso, since it's generally regarded as being better than forward/backward/stepwise selection. Ridge is also another possibility, but it will include all predictors in the model. Ridge and Lasso are techniques that shrink the coefficient estimates to zero, include a tuning parameter and shrinkage penalty. Additionally, Ridge and Lasso are computationally less expensive than forward/backward/stepwise. Chapter 6 of ISLR mentioned above has a really good explanation of the different model selection techniques.

```{r, eval=FALSE}

# New attempt
library(caret)
library(glmnet)
hittersGlm <- hitters %>% select(-c("Salary","CAtBat","CHits","CHmRun","CRuns","CRBI","CWalks"))
set.seed(123)
training.samples <- hittersGlm$logSalary %>% createDataPartition(p = 0.8, list = FALSE)
train.data  <- hittersGlm[training.samples, ]
test.data <- hittersGlm[-training.samples, ]

# Predictor variables
x <- model.matrix(logSalary~., train.data)[,-1]
# Outcome variable
y <- train.data$logSalary

# Find the best lambda using cross-validation
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 0)

# Display the best lambda value
cv$lambda.min

# Fit the final model on the training data
modelRidge <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)

# Display regression coefficients
coef(modelRidge)

# Make predictions on the test data
x.test <- model.matrix(logSalary ~., test.data)[,-1]
predictions <- modelRidge %>% predict(x.test) %>% as.vector()
# Model performance metrics
metricsRidge <- data.frame(
  RMSERidge = RMSE(predictions, test.data$logSalary),
  RsquareRidge = R2(predictions, test.data$logSalary)
)

  
# LASSO
# Find the best lambda using cross-validation
# Note that alpha = 1 here
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 1)

# Display the best lambda value
cv$lambda.min

# Fit the final model on the training data
modelLasso <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)

# Dsiplay regression coefficients
coef(modelLasso)

# Make predictions on the test data
x.test <- model.matrix(logSalary ~., test.data)[,-1]
predictions <- modelLasso %>% predict(x.test) %>% as.vector()
# Model performance metrics
metricsLasso <- data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)

# Elastic Ridge
# Build the model using the training set
set.seed(123)
modelElastic <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)

# Best tuning parameter
modelElastic$bestTune

# Coefficient of the final model. You need to specify the best lambda
coef(modelElastic$finalModel, modelElastic$bestTune$lambda)

# Make predictions on the test data
x.test <- model.matrix(logSalary ~., test.data)[,-1]
predictions <- modelElastic %>% predict(x.test)
# Model performance metrics
metricsElastic <- data.frame(
  RMSE = RMSE(predictions, test.data$medv),
  Rsquare = R2(predictions, test.data$medv)
)


### comparison

lambda <- 10^seq(-3, 3, length = 100)

#Least Squares, just set lambda = 0
set.seed(123)
lSquares <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = 0)
  )
# Model coefficients
coef(lSquares$finalModel, lSquares$bestTune$lambda)
# Make predictions
predictions <- lSquares %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)


#Ridge
set.seed(123)
ridge <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- ridge %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)

# Build the model
set.seed(123)
lasso <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
  )
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)


set.seed(123)
elastic <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Model coefficients
coef(elastic$finalModel, elastic$bestTune$lambda)
# Make predictions
predictions <- elastic %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)

models <- list(leastSquares = lSquares, ridge = ridge, lasso = lasso, elastic = elastic)
resamples(models) %>% summary( metric = "RMSE")


##############################

# Setup for glmnet
library(caret)
hittersGlm <- hitters %>% select(-c("Salary","CAtBat","CHits","CHmRun","CRuns","CRBI","CWalks"))
x=model.matrix(logSalary~.,hittersGlm)[,-1]
y=hittersGlm$logSalary
grid=10^seq(3,-3,length=100)

# Create filter for train/test
set.seed(123)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]

# Build model using training data
lasso.mod=glmnet(x[train,],y[train],alpha =1,lambda=grid)
plot(lasso.mod)

# find lamda that results in smallest cross validation error and the MSE
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha =1)
plot(cv.out)
bestlam =cv.out$lambda.min
bestlam
lasso.pred=predict(lasso.mod ,s=bestlam,newx=x[test ,])

#Output
testMSE_LASSO<-mean((y.test-lasso.pred)^2)
testMSE_LASSO

# Refit Lasso against full data set and output coefficients
out=glmnet(x,y,alpha=1,lambda=grid)
coef(out,s=bestlam)

# William's input

set.seed(123)

library(caret)
lambda <- 10^seq(-3, 3, length = 100)
set.seed(314159)

lasso <- train(
  logSalary ~., data = hittersGlm, method = "glmnet",
  trControl = trainControl("cv", number = 5),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
  )
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predLasso <- lasso %>% predict(hitters)
# Model prediction performance
perfLasso<-data.frame(
  RMSE = RMSE(exp(predLasso), hitters$Salary),
  Rsquare = R2(exp(predLasso), hitters$Salary)
)

# Set Alpha = 0 for ridge
ridge <- train(
  logSalary ~., data = hittersGlm, method = "glmnet",
  trControl = trainControl("cv", number = 5),
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predRidge <- ridge %>% predict(hittersGlm)
# Model prediction performance
perfRidge<-data.frame(
  RMSE = RMSE(exp(predRidge), hitters$Salary),
  Rsquare = R2(exp(predRidge), hitters$Salary)
)

# Set Alpha = 0 for ridge, and set lamda = 0, should give us least squares estimate
leastSquares <- train(
  logSalary ~., data = hittersGlm, method = "glmnet",
  trControl = trainControl("cv", number = 5),
  tuneGrid = expand.grid(alpha = 0, lambda = 0)
  )
# Model coefficients
coef(leastSquares$finalModel, leastSquares$bestTune$lambda)
# Make predictions
predLeastSquares <- leastSquares %>% predict(hittersGlm)
# Model prediction performance
perfLeastSquares<-data.frame(
  RMSE = RMSE(exp(predLeastSquares), hitters$Salary),
  Rsquare = R2(exp(predLeastSquares), hitters$Salary)
)
```




## Two Way Anova

Basic Flow  
    * What Situation are we in?  
    * Plot Data (Mean Profiling)  
    * Fit Full Model with both factors and Interaction  
    * Diagnostics  
        * Residuals  
        * Normality, Independence, Constant Variance  
        * Outliers  
    * Testing  
        * High Level Anova  
        * Contrasts  
        
### What Situation are we in?  

### Mean Profiling  

```{r meanstable}
library(FSA)

Summarize(Salary ~ League + Division,data=hitters)

mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/length(x),min(x),max(x),IQR(x))
  names(result)<-c("N","Mean","SD","SE","Min","Max","IQR")
  return(result)
}
sumstats<-aggregate(Salary~League*Division,data=hitters,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats
```

```{r means plot}
ggplot(sumstats,aes(x=Division,y=Mean,group=League,colour=League))+
  ylab("Salary")+
  geom_line(position = position_dodge(0.6))+
  geom_point(position = position_dodge(0.6))+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1, position = position_dodge(0.6))+
  labs(title = "Salary Means Plot (SD)")
```

The SD bars largely overlap, variance seems difference E vs W but not A vs N. Bars not parallel.

### Fit Full Model  

```{r, message=FALSE}
library(car)
model.fit<-aov(Salary ~ League +Division + League:Division,data=hitters)
Anova(model.fit,type=3)
```

It seems like only Division is significant, not League or the interaction

### Diagnostics  

```{r }
library(gridExtra)
myfits<-data.frame(fitted.values=model.fit$fitted.values,residuals=model.fit$residuals)

#Residual vs Fitted
plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")

grid.arrange(plot1, plot2,plot3, ncol=3)
```

The residuals are non normal and cone shaped. A transformation of Salary seems to be required. I will re-run with a log transform.

### Fit Full Model  

```{r, message=FALSE}
library(car)
model.fit<-aov(logSalary ~ Division + League + Division:League,data=hitters)
Anova(model.fit,type=3)

model.fit<-aov(logSalary ~ League + Division + League:Division,data=hitters)
Anova(model.fit,type=3)


```

Same as pre transform, It seems like only Division is significant, not League or the interaction

### Diagnostics  

```{r }
library(gridExtra)
myfits<-data.frame(fitted.values=model.fit$fitted.values,residuals=model.fit$residuals)

#Residual vs Fitted
plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")

grid.arrange(plot1, plot2,plot3, ncol=3)
```

The residuals are non normal and cone shaped. A transformation of Salary seems to be required. I will re-run with a log transform.
    
### Testing

I think we only need to test division since it was the only signficant factor. Running both for kicks. 

```{r}
TukeyHSD(model.fit,"Division",conf.level=.95) 
plot(TukeyHSD(model.fit,"Division",conf.level=.95)) 

TukeyHSD(model.fit,"League:Division",conf.level=.95) 
plot(TukeyHSD(model.fit,"League:Division",conf.level=.95)) 
```

There is evidence of a difference between East and West of about $766 (exp(-0.2665039)) with a p-value 0.0145.






