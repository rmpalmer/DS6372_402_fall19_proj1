---
title: "Stats2 Project 1"
date: "`r Sys.Date()`"
author: "William Arnost, Dan Crouthamel"
output:
  rmdformats::readthedown:
    highlight: kate
    toc_depth: 3
---


```{r knitr_init, echo=FALSE, cache=FALSE,message=FALSE}
library(knitr)
library(rmdformats)
library(tidyverse)
library(kableExtra)
library(gridExtra)
library(doParallel)

workers <- makeCluster(8L)
registerDoParallel(workers)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=75)
```

# Hitters Data Set  

Source: https://www.kaggle.com/floser/hitters  

The following text is verbatim from the website. It is mostly so I don't have to go look at it constantly, we should delete or rephrase for the final submission.  

This dataset is part of the R-package ISLR and is used in the related book by G. James et al. (2013) "An Introduction to Statistical Learning with applications in R" to demonstrate how Ridge regression and the LASSO are performed using R.  

This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.

Format A data frame with 322 observations of major league players on the following 20 variables. AtBat Number of times at bat in 1986 Hits Number of hits in 1986 HmRun Number of home runs in 1986 Runs Number of runs in 1986 RBI Number of runs batted in in 1986 Walks Number of walks in 1986 Years Number of years in the major leagues CAtBat Number of times at bat during his career CHits Number of hits during his career CHmRun Number of home runs during his career CRuns Number of runs during his career CRBI Number of runs batted in during his career CWalks Number of walks during his career League A factor with levels A and N indicating player’s league at the end of 1986 Division A factor with levels E and W indicating player’s division at the end of 1986 PutOuts Number of put outs in 1986 Assists Number of assists in 1986 Errors Number of errors in 1986 Salary 1987 annual salary on opening day in thousands of dollars NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987  

Please cite/acknowledge: Games, G., Witten, D., Hastie, T., and Tibshirani, R. (2013) An Introduction to Statistical Learning with applications in R, www.StatLearning.com, Springer-Verlag, New York.  

## Dictionary  

Again, not original content, this is from the website I am just making it more readable.  

Format A data frame with 322 observations of major league players on the following 20 variables.  
AtBat Number of times at bat in 1986  
Hits Number of hits in 1986  
HmRun Number of home runs in 1986  
Runs Number of runs in 1986  
RBI Number of runs batted in in 1986  
Walks Number of walks in 1986  
Years Number of years in the major leagues  
CAtBat Number of times at bat during his career  
CHits Number of hits during his career  
CHmRun Number of home runs during his career  
CRuns Number of runs during his career  
CRBI Number of runs batted in during his career  
CWalks Number of walks during his career  
League A factor with levels A and N indicating player’s league at the end of 1986  
Division A factor with levels E and W indicating player’s division at the end of 1986  
PutOuts Number of put outs in 1986  
Assists Number of assists in 1986  
Errors Number of errors in 1986  
Salary 1987 annual salary on opening day in thousands of dollars  
NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987  

## Data Import  

The data has 322 observations and 20 variables. 17 numeric variables and 3 character variables.  
```{r import}
hitters <- read_csv("Hitters.csv")
str(hitters)
head(hitters)
```

## Check for missing data  

Salary seems to be missing for 17ish percent of the data, all other fields are present. I should look at the data dictionary its strange that the dependent variable has that much missing here.  

```{r missing}
library(naniar)
gg_miss_var(hitters, show_pct=TRUE) + labs(title="Percent Missing by Data Field") +
  theme(legend.position = "none",plot.title = element_text(hjust = 0.5),axis.title.y=element_text(angle=0,vjust=1))
```

## Helper Functions  

These are to just make EDA a little faster.  

```{r helper}
barPlot <- function(df = hitters, x) {
  ggplot(df, aes_string(x = x)) +
    geom_bar() 
}

propPlot <- function(df = hitters, x, y) {
  ggplot(df, aes_string(x = x, fill = y)) +
    geom_bar(position = "fill") +
    scale_y_continuous(labels = scales::percent)
}

fancyTable <- function(df = hitters, x) {
  df %>% group_by_at(x) %>% 
    summarise(Count = n(), Proportion = scales::percent(n()/dim(df)[1])) %>% 
    kable() %>% kable_styling(full_width = FALSE)
}

vioPlot <- function(df = hitters, x, y) {
  ggplot(df, aes_string(x = x, y = y, fill = x)) + 
    geom_violin(show.legend = FALSE) + 
    geom_boxplot(width = 0.20, show.legend = FALSE) + 
    stat_summary(fun.y=mean, geom="point",
                 shape=5, size=4, color = "black",
                 show.legend = FALSE)
}

histPlot <- function(df = hitters, x){
  ggplot(df,aes_string(x)) + geom_histogram(bins = 30) 
}

scatterPlot <- function(df = hitters, x, y = "Salary") {
  ggplot(df, aes_string(x = x, y = y)) + geom_point() + geom_smooth(method="lm")
}
```

## Exploratory Data Analysis  

### Pre-Processing  

We can't do any prediction on missing values. I am choosing to eliminate them now. A sub analyis would look at whether or not independent variables of missing observations have a different distribution.  

```{r preprocess}
colsToFactor <- c("League","Division","NewLeague")
hitters[,colsToFactor] <- lapply(hitters[,colsToFactor], as.factor) ## Converting char variables to factors
hitters$Id <- seq(1:nrow(hitters)) # Adding ID column
hitters <- hitters %>% filter(!is.na(Salary)) #Removing rows with no salary info

#logging skewed variables
hitters$logSalary <- log(hitters$Salary)
hitters$logCAtBat <- log(hitters$CAtBat)
hitters$logCHits <- log(hitters$CHits)
hitters$logCHmRun <- log(hitters$CHmRun+1) ## We add 1 to deal with players with zero Homeruns
hitters$logHmRun <- log(hitters$HmRun+1) ## We add 1 to deal with players with zero Homeruns
hitters$logCRuns <- log(hitters$CRuns)
hitters$logCRBI <- log(hitters$CRBI)
hitters$logCWalks <- log(hitters$CWalks)

hitters$outlier <- as.factor(ifelse(hitters$Id %in% c(296,218), 1, 0))


```


### Salary  

Salary is non-normal. May benefit from transformation. 59 NAs. Min Salary ~67.5K, not bad.  

```{r Salary}
a <- ggplot(hitters,aes(Salary)) + geom_histogram(aes(y=..density..), bins = 30) + 
   stat_function(fun=dnorm,
      color="red",
      args=list(mean=mean(hitters$Salary), 
      sd=sd(hitters$Salary))) 
a
summary(hitters$Salary)

b <- ggplot(hitters,aes(logSalary)) + geom_histogram(aes(y=..density..), bins = 30) + 
   stat_function(fun=dnorm,
      color="red",
      args=list(mean=mean(hitters$logSalary), 
      sd=sd(hitters$logSalary))) 
b

c <- vioPlot(x = "League",y="Salary")
c

d <- vioPlot(x = "Division",y="Salary")
d
```
```{r}
grid.arrange(a,b,ncol=2)
```

### AtBat  

Number of times at bat in 1986. Positive influence on Salary.    

```{r AtBat}
histPlot(x="AtBat")
summary(hitters$AtBat)
vioPlot(x = "League",y="AtBat")
vioPlot(x = "Division",y="AtBat")
scatterPlot(x = "AtBat")
```

### Hits  

Hits Number of hits in 1986. Positive influence on Salary    

```{r Hits}
histPlot(x="Hits")
summary(hitters$Hits)
vioPlot(x = "League",y="Hits")
vioPlot(x = "Division",y="Hits")
scatterPlot(x = "Hits")
```

### HmRun  

HmRun Number of home runs in 1986  

```{r HmRun}
histPlot(x="HmRun")
summary(hitters$HmRun)
vioPlot(x = "League",y="HmRun")
vioPlot(x = "Division",y="HmRun")
scatterPlot(x = "HmRun")
```

### Runs    

Runs Number of runs in 1986   

```{r Runs}
histPlot(x="Runs")
summary(hitters$Runs)
vioPlot(x = "League",y="Runs")
vioPlot(x = "Division",y="Runs")
scatterPlot(x = "Runs")
```

### RBI  

RBI Number of runs batted in in 1986   

```{r RBI}
histPlot(x="RBI")
summary(hitters$RBI)
vioPlot(x = "League",y="RBI")
vioPlot(x = "Division",y="RBI")
scatterPlot(x = "RBI")
```

### Walks  

Walks Number of walks in 1986  

```{r Walks}
histPlot(x="Walks")
summary(hitters$Walks)
vioPlot(x = "League",y="Walks")
vioPlot(x = "Division",y="Walks")
scatterPlot(x = "Walks")
```

### Years    

Years Number of years in the major leagues    

```{r Years}
histPlot(x="Years")
summary(hitters$Years)
vioPlot(x = "League",y="Years")
vioPlot(x = "Division",y="Years")
scatterPlot(x = "Years")
```

### CAtBat  

CAtBat Number of times at bat during his career   

```{r CAtBat}
histPlot(x="CAtBat")
summary(hitters$CAtBat)
vioPlot(x = "League",y="CAtBat")
vioPlot(x = "Division",y="CAtBat")
scatterPlot(x = "CAtBat")
```

### CHits   

CHits Number of hits during his career   

```{r CHits}
histPlot(x="CHits")
summary(hitters$CHits)
vioPlot(x = "League",y="CHits")
vioPlot(x = "Division",y="CHits")
scatterPlot(x = "CHits")
```

### CHmRun   

CHmRun Number of home runs during his career   

```{r CHmRun}
histPlot(x="CHmRun")
summary(hitters$CHmRun)
vioPlot(x = "League",y="CHmRun")
vioPlot(x = "Division",y="CHmRun")
scatterPlot(x = "CHmRun")
```

### CRuns   

CRuns Number of runs during his career    

```{r CRuns}
histPlot(x="CRuns")
summary(hitters$CRuns)
vioPlot(x = "League",y="CRuns")
vioPlot(x = "Division",y="CRuns")
scatterPlot(x = "CRuns")
```

### CRBI   

CRBI Number of runs batted in during his career     

```{r CRBI}
histPlot(x="CRBI")
summary(hitters$CRBI)
vioPlot(x = "League",y="CRBI")
vioPlot(x = "Division",y="CRBI")
scatterPlot(x = "CRBI")
```

### CWalks   

CWalks Number of walks during his career     

```{r CWalks}
histPlot(x="CWalks")
summary(hitters$CWalks)
vioPlot(x = "League",y="CWalks")
vioPlot(x = "Division",y="CWalks")
scatterPlot(x = "CWalks")
```

### PutOuts     

PutOuts Number of put outs in 1986     

```{r PutOuts}
histPlot(x="PutOuts")
summary(hitters$PutOuts)
vioPlot(x = "League",y="PutOuts")
vioPlot(x = "Division",y="PutOuts")
scatterPlot(x = "PutOuts")
```

### Assists 

Assists Number of assists in 1986  

```{r Assists}
histPlot(x="Assists")
summary(hitters$Assists)
vioPlot(x = "League",y="Assists")
vioPlot(x = "Division",y="Assists")
scatterPlot(x = "Assists")
```

### Errors  

Errors Number of errors in 1986    

```{r Errors}
histPlot(x="Errors")
summary(hitters$Errors)
vioPlot(x = "League",y="Errors")
vioPlot(x = "Division",y="Errors")
scatterPlot(x = "Errors")
```

### League  

League A factor with levels A and N indicating player’s league at the end of 1986  

```{r League}
fancyTable(x="League")
barPlot(x="League")
vioPlot(x = "League",y="Salary")
```

### Division  

Division A factor with levels E and W indicating player’s division at the end of 1986    

```{r Division}
fancyTable(x="Division")
barPlot(x="Division")
vioPlot(x = "Division",y="Salary")
```

### NewLeague  

NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987    

```{r NewLeague}
fancyTable(x="NewLeague")
barPlot(x="NewLeague")
vioPlot(x = "NewLeague",y="Salary")
```

### Correlation Plot  

```{r correlation}
library(GGally)
#ggpairs(data = hitters, 
#              mapping = aes(color = League),
#              columns = c("Salary","AtBat","Hits","HmRun","Runs","RBI","Walks","Years",
#                          "CAtBat","CHits","CHmRun","CRuns","CRBI","CWalks","PutOuts",
#                          "Assists","Errors"))

# ggpairs(data = hitters, 
#               mapping = aes(color = League),
#               columns = c("Salary","CAtBat","CHits","CHmRun","CRuns","CRBI","CWalks","PutOuts",
#                           "Assists","Errors"))
# 
# ggpairs(data = hitters, 
#               mapping = aes(color = League),
#               columns = c("Salary","AtBat","Hits","HmRun","Runs","RBI","Walks","Years"))

library(corrplot)
cors <- cor(hitters %>% keep(is.numeric) %>% select(c("AtBat","Hits","HmRun","Runs","RBI","Walks","Years","logCAtBat","logCHits","logCHmRun","logCRuns","logCRBI","logCWalks","PutOuts",
"Assists","Errors","logSalary")))
corrplot(cors, method="circle")
```

## Linear Model Selection ##
As mentioned above, we see some justification for logging Salary. Additionally, there is benefit to logging the career attributes as well, those the start with 'C'. Let's compare before and after scatter plots.

* Scatter plots using Salary
```{r pairsplot1, message=FALSE}
ggpairs(data = hitters, columns = c('Salary','CAtBat','CHits','CHmRun','CRuns','CRBI','CWalks'))
```

* Scatter plots using log of Salary
```{r pairsplot2, message=FALSE}
ggpairs(data = hitters, columns = c('logSalary','logCAtBat','logCHits','logCHmRun','logCRuns','logCRBI','logCWalks'),  lower = list(
    mapping = aes(color = outlier)
  ))
```
```{r pairsplot3, message=FALSE}
ggpairs(data = hitters, columns = c('logSalary','AtBat','Hits','HmRun','Runs','RBI','Walks'),  lower = list(
    mapping = aes(color = outlier)
  ))
ggpairs(data = hitters, columns = c('logSalary','AtBat','Hits','logHmRun','HmRun','Runs','RBI','Walks','PutOuts','Assists','Errors'),  lower = list(
    mapping = aes(color = outlier)
  ))
```
* Scatter plots using log of Salary and log of Career Attributes. Important, I'm adding 1 to the log to avoid 0s/NAs.
```{r, message=FALSE}
## Moved this to data processing

##hitters$logCAtBat <- log(hitters$CAtBat+1)
##hitters$logCHits <- log(hitters$CHits+1)
##hitters$logCHmRun <- log(hitters$CHmRun+1)
##hitters$logCRuns <- log(hitters$CRuns+1)
##hitters$logCRBI <- log(hitters$CRBI+1)
##hitters$logCWalks <- log(hitters$CWalks+1)

ggpairs(data = hitters, columns = c('logSalary','logCAtBat','logCHits','logCHmRun','logCRuns','logCRBI','logCWalks'))
```

Linear Model Selection*

The following attempts to compare 4 different models

* Least Squares
* Ridge
* Lasso
* Elastic Net

Ridge, Lasso and Elastic Net are know as shrinkage or methods, that constrain or shrink the regression coefficients to 0. They are a form of penalized regression, where a model is penalized for having too many variables. Ridge will generally perform better than least squares, but includes all predictors. With Lasso, some coefficients will be exactly 0, allowing us to simplify our model. Elastic Net is somewhere in between, includes more variables, but some will be 0.

```{r ModelSelection}

library(caret)
library(glmnet)
library(gridExtra)

# Prep data, create training and test data
# Here it is split 75/25
# If you are interested in not using the log of the C attributes, use the second hittersGlm set below
hittersGlm <- hitters %>% select(-c("Salary","CAtBat","CHits","CHmRun","CRuns","CRBI","CWalks","logHmRun","Id","outlier"))
#hittersGlm <- hitters %>% select(-c("Salary","logCAtBat","logCHits","logCHmRun","logCRuns","logCRBI","logCWalks"))

set.seed(123)
training.samples <- hittersGlm$logSalary %>% createDataPartition(p = 0.75, list = FALSE)
train.data  <- hittersGlm[training.samples, ]
test.data <- hittersGlm[-training.samples, ]

# Predictor variables
x <- model.matrix(logSalary~., train.data)[,-1]
# Outcome variable
y <- train.data$logSalary

# Setup a grid range of lambda values
lambda <- 10^seq(-3, 3, length = 100)

#Least Squares, just set lambda = 0
set.seed(123)
lSquares <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = 0)
  )
# Model coefficients
coef(lSquares$finalModel, lSquares$bestTune$lambda)
# Make predictions
predictions <- lSquares %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)

#Ridge Model
set.seed(123)
ridge <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 0, lambda = lambda)
  )
# Model coefficients
coef(ridge$finalModel, ridge$bestTune$lambda)
# Make predictions
predictions <- ridge %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)

#Lasso Model
set.seed(123)
lasso <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda)
  )
# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
# Make predictions
predictions <- lasso %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)

#Elastic Model
set.seed(123)
elastic <- train(
  logSalary ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
  )
# Model coefficients
coef(elastic$finalModel, elastic$bestTune$lambda)
# Make predictions
predictions <- elastic %>% predict(test.data)
# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, test.data$logSalary),
  Rsquare = R2(predictions, test.data$logSalary)
)

models <- list(leastSquares = lSquares, ridge = ridge, lasso = lasso, elastic = elastic)
resamples(models) %>% summary(metric = "RMSE")
resamples(models) %>% summary(metric = "Rsquared")

library(gridExtra)
# make predictions against full set
fullPredictions <- lasso %>% predict(hittersGlm)
lassoFits<-data.frame(fitted.values=fullPredictions,residuals=hittersGlm$logSalary)

#Residual vs Fitted
lassoPlot1<-ggplot(lassoFits,aes(x=fitted.values,y=residuals))+ylab("Salary Log")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
lassoPlot2<-ggplot(lassoFits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(lassoFits$residuals), slope = sd(lassoFits$residuals))

#Histogram of residuals
lassoPlot3<-ggplot(lassoFits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),color="black", fill="gray", bins=20)+
  geom_density(alpha=.1, fill="red")

#Residual minus Fitted vs Fitted
lassoPlot4<-ggplot(lassoFits,aes(x=fitted.values,y=(residuals-fitted.values)))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

grid.arrange(lassoPlot1, lassoPlot2, lassoPlot3, lassoPlot4, ncol=2, nrow=2)


#Use glm to build model based on lasso, and find intervals
glmModel <- glm(hittersGlm$logSalary ~ hittersGlm$Hits + hittersGlm$Runs + hittersGlm$Walks + hittersGlm$League + hittersGlm$Division + hittersGlm$PutOuts + hittersGlm$Assists + hittersGlm$logCHmRun + hittersGlm$logCRuns + hittersGlm$logCRBI, data=hittersGlm)

#Coefficients
#coef(glmModel)
summary(glmModel)

#Confidence Intervals
confint(glmModel)

# Here I am taking the model proposed by Lasso, and making a prediction on each of the observations.
# Then compare that to the actual value
glmPredict <- predict(glmModel,hittersGlm)
glmASE <- mean((hitters$logSalary-glmPredict)^2)
glmRMSE <- sqrt(glmASE)

# Note, we can also compute RMSE with the call below. Doing the above was an excersise for self, it yields the same result. This is to compare ASE with RMSE.
#glmRMSE <- RMSE(glmPredict,hitters$logSalary)
glmR2 <- R2(glmPredict,hitters$logSalary)

# Here we can see that the R2 is rougly the same, and the RMSE value is bit lower than what we found when comparing models on the test data.

# Finally, let's do a vif call to validate our final model.
library(car)
vif(glmModel)

# Create a new model using just career hits and RBIs
glmSimple <- glm(hittersGlm$logSalary ~ hittersGlm$logCRBI)
glmSimplePredict <- predict(glmSimple,hittersGlm)

glmSimpleRMSE <- RMSE(glmSimplePredict,hittersGlm$logSalary)
glmSimpleR2 <- R2(glmSimplePredict,hittersGlm$logSalary)
summary(glmSimple)
confint(glmSimple)

# Same as above, but remove outliers
hittersNoOut <- hittersGlm[-c(173,241),]
glmSimple <- glm(hittersNoOut$logSalary ~ hittersNoOut$logCRBI)
glmSimplePredict <- predict(glmSimple,hittersNoOut)

glmSimpleRMSE <- RMSE(glmSimplePredict,hittersNoOut$logSalary)
glmSimpleR2 <- R2(glmSimplePredict,hittersNoOut$logSalary)
summary(glmSimple)
confint(glmSimple)
```

We performed a comparison between 4 different models and the mean RMSE value was nearly the same for all 4. Lasso was selected because it resulted in the least number of coefficients, thereby producing a simpler, easier to explain model.

We then took the features selected by Lasso and created a model for it using the entire data set. In this case, the R2 for the model was nearly the same as the other test models, and the RMSE was a bit less too.

Using vif in the car library we found that there is some correlation between predictors, which makes sense. For example, hitting a home run will increase your Run and RBI attributes. We do see some large VIF values. This can indicate that the model can have a hard time estimating the coefficient, it doesn't necessarily degrade the quality of the predictors.

We then tried a simplified model, using just one predictor. In this case, the RMSE was just a bit higher, around .60. Removing outliers, the RMSE goes to .57.

## Two Way Anova

Basic Flow  
    * What Situation are we in?  
    * Plot Data (Mean Profiling)  
    * Fit Full Model with both factors and Interaction  
    * Diagnostics  
        * Residuals  
        * Normality, Independence, Constant Variance  
        * Outliers  
    * Testing  
        * High Level Anova  
        * Contrasts  
        
### What Situation are we in?  

### Mean Profiling  

```{r meanstable}
hitters2WA <- hitters
#hitters2WA <- hitters[-c(173,241),]
library(FSA)

Summarize(Salary ~ League + Division,data=hitters2WA)
Summarize(logSalary ~ Division,data=hitters2WA)

mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/length(x),min(x),max(x),IQR(x))
  names(result)<-c("N","Mean","SD","SE","Min","Max","IQR")
  return(result)
}
sumstats<-aggregate(logSalary~League*Division,data=hitters2WA,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats

sumstats2<-aggregate(logSalary~Division,data=hitters2WA,mysummary)
sumstats2<-cbind(sumstats2[,1:2],sumstats2[,-(1:2)])
sumstats2
```

```{r means plot}
ggplot(sumstats,aes(x=Division,y=Mean,group=League,colour=League))+
  ylab("Salary")+
  geom_line(position = position_dodge(0.6))+
  geom_point(position = position_dodge(0.6))+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1, position = position_dodge(0.6))+
  labs(title = "Log Salary Means Plot (SD)")
```

The SD bars largely overlap, variance seems difference E vs W but not A vs N. Bars not parallel.

### Fit Full Model  

```{r, message=FALSE}
library(car)
model.fit<-aov(Salary ~ League +Division + League:Division,data=hitters2WA)
Anova(model.fit,type=3)
```

It seems like only Division is significant, not League or the interaction

### Diagnostics  

```{r }
library(gridExtra)
myfits<-data.frame(fitted.values=model.fit$fitted.values,residuals=model.fit$residuals)

#Residual vs Fitted
plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")

grid.arrange(plot1, plot2,plot3, ncol=3)
```

The residuals are non normal and cone shaped. A transformation of Salary seems to be required. I will re-run with a log transform.

### Fit Full Model  

```{r, message=FALSE}
library(car)
model.fit<-aov(logSalary ~ Division + League + Division:League,data=hitters2WA)
Anova(model.fit,type=3)

model.fit<-aov(logSalary ~ League + Division + League:Division,data=hitters2WA)
Anova(model.fit,type=3)


```

Same as pre transform, It seems like only Division is significant, not League or the interaction

### Diagnostics  

```{r }
library(gridExtra)
myfits<-data.frame(fitted.values=model.fit$fitted.values,residuals=model.fit$residuals)

#Residual vs Fitted
plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")

grid.arrange(plot1, plot2,plot3, ncol=3)
```

The residuals are non normal and cone shaped. A transformation of Salary seems to be required. I will re-run with a log transform.
    
### Testing

I think we only need to test division since it was the only signficant factor. Running both for kicks. 

```{r}
TukeyHSD(model.fit,"Division",conf.level=.95) 
plot(TukeyHSD(model.fit,"Division",conf.level=.95)) 

TukeyHSD(model.fit,"League:Division",conf.level=.95) 
plot(TukeyHSD(model.fit,"League:Division",conf.level=.95)) 
```

There is evidence of a difference between East and West of about $766 (exp(-0.2665039)) with a p-value 0.0145.



```{r}
levels(hitters2WA$Division)

library(lsmeans)
leastsquare = lsmeans(model.fit, "Division")

Contrasts = list(WvsE = c(-1, 1))
myconstrast <- contrast(leastsquare, Contrasts, adjust="none")
myconstrast
confint(myconstrast)
exp(-0.266)*1000
exp(-0.48)*1000
exp(-0.0525)*1000
```

```{r}
Summarize(logSalary ~ Division,data=hitters2WA)
model.fit<-aov(logSalary ~ Division ,data=hitters2WA)
Anova(model.fit,type=3)
confint(model.fit)
leastsquare = lsmeans(model.fit, "Division")
leastsquare %>% kable() %>% kable_styling(full_width = FALSE)
Contrasts = list(WvsE = c(-1, 1))
contrast(leastsquare, Contrasts, adjust="none") %>% kable() %>% kable_styling(full_width = FALSE)

```


